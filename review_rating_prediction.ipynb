{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will work on a real world dataset from Yelp. The ultimate goal of your job is to predict the customer rating for businesses like restaurants or bars based on the customer review text.\n",
    "\n",
    "During this assignment, you will need to build a machine learning pipeline to achive this goal by yourself. But don't worry, this notebook will guide you through those process step by step. To be specific, the tasks in this assignment include:\n",
    "- Data Pre-processing and Exploratory Data Analysis (EDA)\n",
    "- Basic Feature Engineering\n",
    "- Model Design and Implementation\n",
    "- Model Evaluation\n",
    "\n",
    "Moreover, at the end of this assignment, we will also let you use the popular machine learning library scikit-learn to build the similar model.\n",
    "\n",
    "Hopefully, after this assignment you can know the general approach to solve a text related machine learning problem, get a sense on what kind of challenges you may encounter when working on the real world datasets and understand how the machine learning library can boost your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get familiar with the datasets, you should find 6 files:\n",
    "\n",
    "|Description|Filename|\n",
    "|---|---|\n",
    "|Training Set| reviews_train.json  |\n",
    "|Validation Set   | reviews_dev.json  |\n",
    "| Test Set  | reviews_test.json  |\n",
    "|Stopword list file|stopword.list|\n",
    "|True lables for reviews in validation set|reviews_dev_labels.txt|\n",
    "|True lables for reviews in test set|reviews_test_labels.txt|\n",
    "\n",
    "The data in training, validation and test sets are stored in JSON format. Each line of the file is a JSON object. The content of JSON is shown as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'type': 'review',\n",
    "    'review_id': (encrypted review id),\n",
    "    'business_id': (encrypted business id),\n",
    "    'user_id': (encrypted user id),\n",
    "    'stars': (star rating), # Notice: this field is only available to training set.\n",
    "    'text': (review text),\n",
    "    'date': (date, formatted like 'YYYY-MM-DD'),\n",
    "    'votes': {(vote type): (count)},\n",
    "}\n",
    "```\n",
    "\n",
    "In this assignment, we will focus on using text field (the review content) only to predict the rating stars (the stars field). The review_id field will be used to match the true label in validation and test set.\n",
    "\n",
    "Note, the true labels for reviews in test set will be released after you finish this assignment, which will be used for final evaluation of the models you built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, open the each file and have a look, then move to the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first import some useful Python library first\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set the path for each file, they should be placed in the same folder as this notebook\n",
    "file_stopword = 'stopword.list'\n",
    "\n",
    "file_train = 'reviews_train.json'\n",
    "\n",
    "file_dev = 'reviews_dev.json'\n",
    "file_dev_label = 'reviews_dev_labels.txt'\n",
    "\n",
    "file_test = 'reviews_test.json'\n",
    "file_test_label = 'reviews_test_labels.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing and Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first task, we prepare the training set with the following steps:\n",
    "- Load the stopwords from file and store them in a set, which will be used to filter out non-useful words\n",
    "- Extract the useful fields from the training set\n",
    "- Tokenize the text field and remove the stop words\n",
    "- Calculate some statistics to verify your implementation.\n",
    "\n",
    "Here are the requirements for tokenization:\n",
    "- Turn all the tokens into lower case.\n",
    "- Remove all the punctuation in the tokens (e.g., “it’s” will become “its”, “?” or “!” will become “”).\n",
    "- Remove all the tokens that contain numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have implemented the stopwords loading for you\n",
    "def load_stopwords(infile: str) -> set:\n",
    "    with open(infile) as g:\n",
    "        stopwords = set(map(lambda x:x.strip(), g.readlines()))\n",
    "    print(f'{len(stopwords)} stopwords loaded')\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 stopwords loaded\n"
     ]
    }
   ],
   "source": [
    "# Now let's load the stopwords as global variables here\n",
    "stopwords = load_stopwords(file_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the parse_text function, the function takes one comment string\n",
    "#       and return a list of tokens(words)\n",
    "def parse_text(text: str) -> List[str]:\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    \n",
    "    # TODO:\n",
    "    # 1. convert all tokens into lower case\n",
    "    # 2. remove tokens that in stopwords\n",
    "    # 3. remvoe empty token like ''\n",
    "    tokens = [x.strip().lower() for x in text.split(' ') if x and x not in stopwords] # list comprehension 列表推导式\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete preprocess function to load training/validation/test data from files\n",
    "#       if label_file is not None, you should read label from the label file, \n",
    "#       otherwise, load label from data file, \"text\" field in each json object. \n",
    "def preprocess(data_file: str, label_file: str=None) -> Tuple[List[List[str]], List[int]]:\n",
    "    review_list, star_list = [], []\n",
    "    \n",
    "    reviews = {}\n",
    "    stars = {}\n",
    "    with open(data_file, 'r') as f:\n",
    "        for json_str in f:\n",
    "            obj = json.loads(json_str)\n",
    "            # TODO: \n",
    "            # 1. load the reivew_id and reivew text from JSON obj, which\n",
    "            #   is a dictionary after loading to Python\n",
    "            # 2. using the parse_text function to process the review text\n",
    "            # 3. save the token list to review map: {review_id => tokens}\n",
    "            review_id = obj[\"review_id\"]\n",
    "            review = obj[\"text\"]\n",
    "            \n",
    "            tokens = parse_text(review)\n",
    "            reviews[review_id] = tokens\n",
    "            \n",
    "            if label_file is None:\n",
    "                # TODO:\n",
    "                # 1. load the label (stars) from JSON obj\n",
    "                # 2. save tokens and star to output list (i.e., review_list, start_list)\n",
    "                star = int(obj[\"stars\"])\n",
    "                review_list.append(tokens)\n",
    "                star_list.append(star)\n",
    "    \n",
    "    if label_file:\n",
    "        # so we need to manually load stars from file\n",
    "        # file format: review_id<space>stars\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            review_id, star = line.strip().split()\n",
    "            stars[review_id] = int(star)\n",
    "\n",
    "        for review_id, tokens in reviews.items():\n",
    "            review_list.append(tokens)\n",
    "            star_list.append(stars[review_id])\n",
    "    \n",
    "    return review_list, star_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the training dataset, you should get 1.25M records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list, star_list = preprocess(file_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1255353\n"
     ]
    }
   ],
   "source": [
    "# the length of review_list and star_list should match\n",
    "assert len(review_list) == len(star_list)\n",
    "\n",
    "print(f'Number of training samples: {len(review_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the training dataset, find out top 9 most frequent tokens and corresponding counts in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# TODO: implement this function to get the top 9 most frequent tokens\n",
    "def get_top_token_counts(review_list: List[List[str]]) -> List[Tuple[str, int]]:\n",
    "    \n",
    "    token_counter = Counter()\n",
    "    \n",
    "#     # meet expectation\n",
    "#     n = 0\n",
    "#     for tokens in review_list:\n",
    "#         token_counter.update(tokens)\n",
    "#         n += 1\n",
    "#         if n % 100000 == 0:\n",
    "#             print(n)\n",
    "    \n",
    "    # Exceed expectation\n",
    "    for tokens in tqdm(review_list):\n",
    "        token_counter.update(tokens)\n",
    "            \n",
    "    token_cnt_list = sorted(token_counter.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    for token, cnt in token_cnt_list[:9]:\n",
    "        print(f'token={token}\\tcount={cnt}')\n",
    "    \n",
    "    # We will write the token and its count to a file just\n",
    "    # in case you need it :)\n",
    "    with open('token_counts.txt', 'w') as f:\n",
    "        for token, count in token_cnt_list:\n",
    "            f.write(f'{token} {count}\\n')\n",
    "    return token_cnt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1255353/1255353 [00:10<00:00, 125471.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token=i\tcount=3855203\n",
      "token=the\tcount=1255431\n",
      "token=good\tcount=721552\n",
      "token=place\tcount=706729\n",
      "token=food\tcount=673892\n",
      "token=great\tcount=573051\n",
      "token=like\tcount=542963\n",
      "token=just\tcount=518714\n",
      "token=time\tcount=433290\n"
     ]
    }
   ],
   "source": [
    "token_cnt_list = get_top_token_counts(review_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like this, it doesn't matter if the numbers are not exactly matched, as long as they are  close:\n",
    "\n",
    "|Rank |Token|Count|\n",
    "|---|---|---|\n",
    "|1|good|721552|\n",
    "|2|place|706729|\n",
    "|3 |food|673892|\n",
    "|4 |great|573051|\n",
    "|5 |like|542963|\n",
    "|6 |just|518714|\n",
    "|7 |time|433290|\n",
    "|8 |service|411813|\n",
    "|9 |really|385354|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get something like this, it doesn't matter if the numbers are not exactly matched, as long as they are  close:\n",
    "\n",
    "|Rank |Token|Count|\n",
    "|---|---|---|\n",
    "|1|good|721552|\n",
    "|2|place|706729|\n",
    "|3 |food|673892|\n",
    "|4 |great|573051|\n",
    "|5 |like|542963|\n",
    "|6 |just|518714|\n",
    "|7 |time|433290|\n",
    "|8 |service|411813|\n",
    "|9 |really|385354|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's find distribution of user assigned stars, i.e., the number of training instances with 1 star, 2 stars, 3 stars, 4 stars and 5 stars respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this function to get the star distribution on traning set\n",
    "def get_label_counts(star_list: List[int]) -> List[int]:\n",
    "    stars_count = [0] * 5 # 1-5 stars\n",
    "    for star in star_list:\n",
    "        stars_count[star-1] += 1\n",
    "\n",
    "    \n",
    "    print('star counts:', stars_count)\n",
    "    print('distribution:', np.array(stars_count, dtype=float) / sum(stars_count))\n",
    "    return stars_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "star counts: [128038, 112547, 178215, 373469, 463084]\n",
      "distribution: [0.10199362 0.08965367 0.14196405 0.29750118 0.36888748]\n"
     ]
    }
   ],
   "source": [
    "stars_count = get_label_counts(star_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(labels, values, title, ylabel):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(labels, values)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_bar_chart(\n",
    "    labels=['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars'],\n",
    "    values=np.array(stars_count, dtype=float) / sum(stars_count),\n",
    "    title='Label (stars) distribution',\n",
    "    ylabel='Percentage'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you find something unexpected from the distribution? Will this be problematic in training the model? If so, could you try to find some idea about how to address it and understand why the idea should work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait!! It's not over yet!\n",
    "\n",
    "Please feel free to try more things to help you understand the datasets, and we encourage you to get familiar with `matplotlib` library, which is very useful in doing visualization in Python. Visualization is a better way to explain what you found in data than words, so you should try to use various types of visualization to present your findings throughout this entire course. To get more visualization ideas, please check out [Matplotlib Gallery](https://matplotlib.org/gallery/index.html), which also includes the source code for implmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will design and construct the feature for the machine learning models. Since we are using review text for prediction, the generated text tokens for each training instance will be the source for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words (BoW) feature\n",
    "\n",
    "As a baseline approach, let us use a bag-of-words to represent each document, i.e., using a bag of tokens and the corresponding counts in each review, and ignore the order of tokens. For this approach, we need to first define a dictionary. In this baseline model, you can take up to 500 frequent tokens as dictionary and map all the training instances into sparse feature vectors.\n",
    "\n",
    "So how to encode the feature using dictionary and token count? For example, if your dictionary is [a, b, c, d, e, f, g], and a training instance contains tokens {a:2, b:2, d:1, g:1}, your feature vector will be [2, 2, 0 ,1 ,0, 0, 1].\n",
    "\n",
    "Note that the sequence of token in dictionary really doesn’t matter, but you should make sure the sequence if the same for all the instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select top 500 frequent tokens as our dictionary\n",
    "DICT_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the feature map where each token is mapped to a fixed index between 0-499.\n",
    "#       Also, make sure the token_cnt_list is sorted when you slice the top DICT_SIZE\n",
    "def create_feat_map(token_cnt_list: List[Tuple[str, int]], dict_size: int) -> Dict[str, int]:\n",
    "    feat_map = {}\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return feat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_map = create_feat_map(token_cnt_list, DICT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the feature extraction function to build feature vector from review tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the feature exatraction function to construct feature\n",
    "#       vector for the given review list\n",
    "def extract_feature(feat_map: Dict[str, int], review_list: List[List[str]]) -> np.ndarray:\n",
    "    n_sample, n_feat = \n",
    "    X = np.zeros((n_sample, n_feat))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = extract_feature(feat_map, review_list), star_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will also need to get the feature for validation set\n",
    "review_list, star_list = preprocess(file_dev, file_dev_label)\n",
    "X_val, y_val = extract_feature(feat_map, review_list), star_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to predict the ratings (from 1 star to 5 stars) over items (restaurants, shops, pharmacies, etc.) based on the review text from each user on that item.\n",
    "\n",
    "We can consider this as a multi-class classification problem where the model takes the feature vector of each review (which we constructed in the previous part) as the input, and predicts the rating (among 1 to 5) as the class label for the item being reviewed.\n",
    "\n",
    "Specifically, we want you to implement, train and evaluate a regularized multi-class logistic regression (RMLR) method on the training set and validation set we provided, and finally report your results on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first recap the Multi-class Logistic Regression.\n",
    "\n",
    "Similar to binary logistic regression, for the multi-class version of logistic regression we need to 1) formulate the objective function, and 2) derive the update rule for iterative improvement of model parameters (regression coefficients) assuming that we are using gradient ascent (or descent).\n",
    "\n",
    "Letting $c\\in \\{1,2,3,...,C\\}$ be the class-label indicator, we estimate the probability of the class as:\n",
    "\n",
    "$$P(y=c|x;W)=\\frac{e^{w_c^Tx}}{\\sum_{c'=1}^C e^{w_{c'}^Tx}}$$\n",
    "\n",
    "where 𝒙 is the feature vector of a review, $y\\in\\{1,2,3,...C\\}$ is a class label, and $W$ is the model-parameter matrix whose columns are $w_c$for$c\\in \\{1,2,3,...,C\\}$.\n",
    "\n",
    "Now, let's implement the function to make prediction based on the parameter $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first define our model parameter W and initialize it uniformly\n",
    "W = np.ones((5, DICT_SIZE)) / DICT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this function to make prediction on one single instance, the output should be\n",
    "#       the probability of all different class (e.g., [0.1, 0.1, 0.6, 0.15, 0.05])\n",
    "def predict(W: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
    "    scores = None\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since each class has the same weight, your function should output \n",
    "# equal probability for any sample (e.g., [0.2 0.2 0.2 0.2 0.2])\n",
    "fake_x = np.zeros(DICT_SIZE)\n",
    "print(predict(W, fake_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a training set of labeled pairs $D=\\{(x_i, y_i)\\}_{i=1}^n$, we optimize the model parameters as:\n",
    "\n",
    "$$W^* = \\arg\\max_W \\prod_{i=1}^n P(y_i|x_i;W)$$\n",
    "\n",
    "Notice that we assume all the training data are independent and identically distributed (i.i.d.), so we use a product of the probabilities over individual training pairs.\n",
    "\n",
    "When we actually implement the algorithm, we often choose to maximize the log of the conditional likelihood (or minimize the negative loglikelihood). This is because computing the product of probabilities in log space will be more stable and the calculations are also easier. Also, we usually add the regularization terms (we use L2 regularization in this assignment) to prevent overfitting. As a result, we can write our regularized loss function as:\n",
    "\n",
    "$$L(W)=-\\frac{1}{n}\\sum_{i=1}^n log P(y_i|x_i; W)+\\frac{\\lambda}{2}\\sum_{i=1}^C\\|w_i\\|^2$$\n",
    "\n",
    "where λ is a pre-specified parameter that controls the weight of the regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the gradient of loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to derive the gradient of our objective function (the regularized\n",
    "conditional log-likelihood), it is convenient to redefine the output label of i-th input (i.e., $y_i$ ) using a indicator vector $y_i=(y_{i1}, y_{i2},..., y_{iC})^T$, whose elements are defined as\n",
    "\n",
    "$$y_{ic}=\\begin{cases}\n",
    "1,& \\text{if } y_i=c\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the function to convert y=5 to indicator vector y_ind=[0,0,0,0,1]\n",
    "def convert_indicator(y: int) -> np.ndarray:\n",
    "    y_ind = None\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    return y_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convert_indicator(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the conditional probability of $y_i$ given $x_i$ and $W$ can be written as:\n",
    "$$P(y_i|x_i;W)=\\prod_{c=1}^C \\bigg(\\frac{e^{w_c^Tx_i}}{\\sum_{c'=1}^C e^{w_{c'}^Tx_i}} \\bigg)^{y_{ic}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want you to prove that the gradient of $L(W)$ with respect to vector $w_c$ (i.e., $\\frac{\\partial L(W)}{\\partial w_c}$) is equal to:\n",
    "\n",
    "$$-\\frac{1}{n}\\sum_{i=1}^n \\bigg(y_{ic}-\\frac{e^{w_c^Tx_i}}{\\sum_{c'=1}^C e^{w_{c'}^Tx_i}} \\bigg)x_i + \\lambda w_c$$\n",
    "\n",
    "Notice that the gradient of log-likelihood function with respect to a vector $w_c$ is itself a vector, whose i-th element is defined as $\\frac{\\partial L(W)}{\\partial w_{ci}}$, where $w_{ci}$ is the i-th element of vector $w_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, please try to find the gradient from the log-likelihood function, you should derive this step by step on some paper, this process is very necessary, and is asked in many interviews, also the result will be soon used in your own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement the function to calculate gradient for W with a single sample\n",
    "#      so ignore the regularization part (\\lambda * w_c) for now\n",
    "def get_gradient(W: np.ndarray, x: np.ndarray, y: int) -> np.ndarray:\n",
    "    grad = None\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent, Stochastic Gradient Descent and Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard approach to the training of logistic regression is to use Gradient Descent (notice that the goal of our problem is to maximize the regularized log-likelihood, so we will actually use gradient ascent instead of gradient descent). Notice that when the training set is very large, it would be computational expensive to compute the gradient on the entire training set for each iteration. A remedy is to use Stochastic Gradient Descent (SGD) instead, which means to update the model parameters using the gradient on only one training instance per iteration.\n",
    "\n",
    "Generally speaking, the SGD will converge much slower than GD. A midpoint alternative is to use Batched SGD, which means to divide the training set into equally sized subsets (e.g., 100 instances per subset) and to compute the gradient on each subset per iteration. Clearly, there is a trade-off between the number of iterations and the cost of computing the gradient per iteration.\n",
    "\n",
    "Let the learning rate be $\\alpha$, pick the mini-batch GD for your implementation and outline the algorithm. You should specify the update rule for the model parameters ($W$ or $w_c$) in each iteration, how to check the convergence of the solution, the stop criterion and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first implement the gradient update function for a single batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this function to perform one iteration of gradient descent\n",
    "#       on a given batch of data, where lr is the learning rate and regul \n",
    "#       is the regularization parameter\n",
    "def grad_update(W: np.ndarray, X_batch: List[np.ndarray], y_batch: List[int],\n",
    "                lr: float=0.01, regul:float=0.1) -> np.ndarray:\n",
    "\n",
    "    # the gradient should have the same shape as W\n",
    "    batch_grad = np.zeros_like(W)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help make sure your implementation is correct, we also need to implement the evaluation function to verify the training process is moving towards the direction we want. Now let's implement the function to caculate the log-loss (the direct goal you are optimizing) for the given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this function to compute accuracy for a given dataset (X, y)\n",
    "def compute_accuracy(W: np.ndarray, X: np.ndarray, y: List[int]) -> None:\n",
    "    err = 0\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print(f'Accuracy is {1.0 - err / len(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this function to compute log-loss for a given dataset (X, y)\n",
    "def compute_loss(W: np.ndarray, X: np.ndarray, y: List[int], regul: float) -> None:\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    print(f'Loss is {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!! Now let's put everything together and implement the training function for RMLR. During the training process, you will implement the mini-batch gradient descent, use the function you implemented in the previous section to complete the training function.\n",
    "\n",
    "\n",
    "Some tips here:\n",
    "- Think about what batch size you could use for the training, and try your ideas. Note that when batch size equals to the number of training data, the mini-batch gradient descent will become the standard gradient descent\n",
    "- To help make sure the trainig in on the right direction, you should check the value of loss function on training dataset, and the accuracy on validation dataset regularly during the training (e.g., after certain number of batches)\n",
    "- Think about what stopping criteria you should use for the training\n",
    "- You may need to shuffle the order of training samples each time you used the entire dataset, this can help make sure the each batch will always be different\n",
    "- After you figure out the training process, you can tune the learning rate and regularization parameter based on your understanding about how the training works (e.g., if you found your model can get stuck at some point very easily, how about using a dynamic learning rate?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the tips above to complete this function for mini-batch GD model trainig\n",
    "def train_RMLR(X_train: np.ndarray, y_train: List[int], batch_size: int,\n",
    "               lr: float=0.01, regul:float =0.1) -> np.ndarray:\n",
    "    print('Start training RMLR...')\n",
    "    \n",
    "    # randomly initialize W, you can use np.random.rand\n",
    "    W = None\n",
    "    \n",
    "    print('Initial performance')\n",
    "    compute_loss(W, X_train, y_train, regul)\n",
    "    compute_accuracy(W, X_val, y_val)\n",
    "    \n",
    "    n_batch = 0\n",
    "    batch_X = []\n",
    "    batch_y = []\n",
    "    \n",
    "    random_seq = [i for i in range(len(X_train))]\n",
    "    \n",
    "    while True: # add your stop criteria here\n",
    "        for idx in random_seq:\n",
    "            # construct your mini-batch and start training\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    print('Training completed successfully!')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = train_RMLR(X_train, y_train, batch_size=100, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have got the model (i.e., your weight matrix $W$), congratulations! Let's do some evaluation to understand the performance of your model. We will use two metrics (hard one and soft one) to evaluate your rating prediction results.\n",
    "\n",
    "First, let's make predictions using your model for both training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = [predict(W, x).argmax()+1 for x in X_train]\n",
    "y_pred_val = [predict(W, x).argmax()+1 for x in X_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard metrics\n",
    "\n",
    "The hard metric is the overall Accuracy (under zero/one loss) for your multi-class classifier, which is defined as the number of instances that you correctly predict its star divided by the size of the dataset. In addition, you may also want to compute the per-class accuracy to under stand your model's performance in different classes. \n",
    "\n",
    "Let's first implement the per-class accuracy and overall accuracy metrics in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the per-class accuracy and overall accuracy metrics, note\n",
    "#       here the input is true label list and predicted label list\n",
    "def multiclass_accuracy_score(y_true: List[int], y_pred: List[int]) -> None:\n",
    "    count = Counter()\n",
    "    err = Counter()\n",
    "    n_count, n_err = 0, 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'Overall accuracy: {1-n_err/n_count}')\n",
    "    print(f'Per-class accuracy:')\n",
    "    for star, cnt in sorted(count.items()):\n",
    "        print(f'Star={star}, accuracy={1-err[star]/cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy score on both training and validation sets, does the result match your expectation? (<span style=\"color:red\">**You should at least get an overall accuracy score close to 0.5 on validation set**</span>, otherwise please go back and try to improve your model.) What else do you find? Can you think about the potential reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_accuracy_score(y_train, y_pred_train)\n",
    "multiclass_accuracy_score(y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Metric\n",
    "\n",
    "The soft metric is the Root Mean Square Error (RMSE) for your prediction results, which is defined as\n",
    "\n",
    "$$\\text{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(\\hat{r}_i-r_i)^2}$$\n",
    "\n",
    "where $\\hat{r}_i$ is your predicted rating for i-th instance, $r_i$ is the corresponding true rating and $n$ is the size of dataset used for testing. The soft metric shows the average difference between your predictions and true ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the function to evaluate RMSE, here the input is also\n",
    "#       two list of labels (true & predicted)\n",
    "def root_mean_square_err(y_true: List[int], y_pred: List[int]) -> None:\n",
    "    s = 0.0\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'RMSE={np.sqrt(s / len(y_true))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check the RMSE on both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_mean_square_err(y_train, y_pred_train)\n",
    "root_mean_square_err(y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please note: building a good machine learning model is an iterative process\n",
    "\n",
    "It's quite normal if you find something wrong during evaluation or if you are not satisfied with your model's result, always feel free to go back check your implementation (if you think there may be a bug), tune the model parameters (e.g., learning rate, batch_size, regularization hyper parameter, etc.) and try again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Ideas for Model Performance Improvement (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are lacking ideas of how to further improve your model, you may try following ideas\n",
    "\n",
    "- Rethink about the feature engineering part, where we took the top 500 frequent tokens. Do all these tokens equally matter, I don't think so. You may want to take a look and think about how to improve the token list, for example, adj, adv, verb may be more important than noun?\n",
    "- Let's try some advanced optimization approach to replace gradient descent, let's implement the [Adam](https://zhuanlan.zhihu.com/p/32626442) algorithm to our model!!! If you find that's too hard for you, you can pick any one you like other than gradient descent to implement.\n",
    "- Training samples are biased, let's fix it! You can try to use some sampling techniques to help. If you have no idea, do some research then ;)\n",
    "- Think the feature engineering again, do all the reviews have the same of tokens? Probably not, how do you address this? Also, we are now simply dropping all the tokens that are not in our pre-selecte list during feature engineering, which apperantly loses some information? Can you help with that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are good with your own implementation, you can move to the next part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning toolkit: [scikit-learn (sklearn)](https://scikit-learn.org/stable/index.html)\n",
    "\n",
    "As a Data Scientist/Analyst, in most of the cases, you don't really need to implement the machine learning algorithm by yourself, there are many very good machine learning libraries in Python that have already implemented most of the standard machine learning models and scikit-learn is one of the most popular ones among them.\n",
    "\n",
    "However, this doesn't mean implementing a model like what we just did is useless. Implementing a model by yourself is the best way to understand how the model works, and all the experiences you learned will help you use the machine learning library better.\n",
    "\n",
    "*Before we start, if you are not familiar with scikit-learn at all, please click the link in title and have a tour on its official website. There are some tutorials on their website to help you quickly understand how the library works.*\n",
    "\n",
    "Now let's use scikit-learn's Logistic Regression library to train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand the mearning of each parameter\n",
    "LogisticRegression?\n",
    "# You can also visit their website to find the explanation of parameters online:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l2', tol=0.0001, C=1.0, fit_intercept=True, solver='lbfgs', max_iter=30, \n",
    "                           multi_class='auto', verbose=1, n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model and its parameter, let's train it using `.fit(...)` function, you will need to pass the trainig data and labels to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training finishes, you can make prediction using model's `.predict(...)` function, you will also need to pass the test data, which should have exactly the same format as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's do some evaluation using metrics from scikit-learn's library.\n",
    "\n",
    "If you are not familiar with these metrics, please go and explore it on scikit-learn's website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model using sklearn APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trainig accuracy is {accuracy_score(y_train, y_hat_train)}')\n",
    "print(f'Validation accuracy is {accuracy_score(y_val, y_hat_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE & RMSE\n",
    "Sklean only provides API for MSE, but you can take the square root yourself to get RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Trainig RMSE is {np.sqrt(mean_squared_error(y_train, y_hat_train))}')\n",
    "print(f'Validation RMSE is {np.sqrt(mean_squared_error(y_val, y_hat_val))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set')\n",
    "disp = plot_confusion_matrix(model, X_train, y_train, cmap=plt.cm.Blues, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation set')\n",
    "disp = plot_confusion_matrix(model, X_val, y_val, cmap=plt.cm.Blues, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set')\n",
    "print(classification_report(y_train, y_hat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation set')\n",
    "print(classification_report(y_val, y_hat_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have reached the end of this assignment! Congratulations! In this assignment you have built a machine learning model from scratch for solving a real world problem, and the general approach is aligned with what we have introduced during the class!\n",
    "\n",
    "You should also have a good sense on how the machine learning library can help boost our modeling work! In most of other assignments, we will primarily use the machine learning library instead of building the wheels ourselves, then the challenge will become how you can use the library well :)\n",
    "\n",
    "Note: finishing this assignment completely is very crucial as a few other assignments are built on top of the same problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some helper function, to help get you start quickly, we have provided the `token_count_list.txt` which gives the (token, # of appearance) pair from the full training dataset, you can use it to help optimize your features.\n",
    "\n",
    "You may need to execute the first 6 code blocks in this notebook before starting this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count_list():\n",
    "    token_count_list = []\n",
    "    with open('token_count_list.txt', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            token, count = line.strip().split()\n",
    "            token_count_list.append((token, int(count)))\n",
    "    return token_count_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cnt_list = get_token_count_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_cnt_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_SIZE = FEAT_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide this function to build feature map and inverse feature map, so\n",
    "# we can look up index by token and also look up token by index\n",
    "def create_feat_map(token_cnt_list: List[Tuple[str, int]], dict_size: int) -> Dict[str, int]:\n",
    "    token_cnt_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_tokens = [x[0] for x in token_cnt_list[:dict_size]]    \n",
    "\n",
    "    feat_map, inv_feat_map = {}, {}\n",
    "    for i, token in enumerate(top_tokens):\n",
    "        feat_map[token] = i\n",
    "        inv_feat_map[i] = token\n",
    "    return feat_map, inv_feat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_map, inv_feat_map = create_feat_map(token_cnt_list, DICT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we provide the extraction function to build feature vector from review tokens, but note here we made some change:\n",
    "- normalize the feature vector to account for different review length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(feat_map: Dict[str, int], review_list: List[List[str]], normalize=True) -> np.ndarray:\n",
    "    n_sample, n_feat = len(review_list), FEAT_SIZE\n",
    "    X = np.zeros((n_sample, n_feat))\n",
    "    for i, tokens in enumerate(review_list):\n",
    "        feat = X[i]\n",
    "        for token in tokens:\n",
    "            if token in feat_map:\n",
    "                feat[feat_map[token]] += 1\n",
    "        if normalize and np.sum(X[i]) > 0:\n",
    "            X[i] /= np.sum(X[i])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the training & validation dataset\n",
    "review_list, star_list = preprocess(file_train)\n",
    "review_list_val, star_list_val = preprocess(file_dev, file_dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only need training dataset for feature selection\n",
    "X_train, y_train = extract_feature(feat_map, review_list), star_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection using L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use the training dataset and L1 regularization to do feature selection. The idea is in linear model, when the input is properly normalized, the absolute value of weights indicates the relative importance of a feature, so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l1', tol=0.01, C=1.0, fit_intercept=True, solver='saga', max_iter=10, \n",
    "                           multi_class='auto', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the model parameters\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Show the model parameters\n",
    "feat_weights = defaultdict(list)\n",
    "for star, weights in enumerate(model.coef_):\n",
    "    for i, w in enumerate(weights):\n",
    "        token = inv_feat_map[i]\n",
    "        feat_weights[star].append((token, w))\n",
    "    feat_weights[star].sort(key=lambda x: abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top token for each star\n",
    "for star, values in feat_weights.items():\n",
    "    print(f'Star={star+1}', 'key tokens:')\n",
    "    print(values[:20])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the key tokens make sense to you?\n",
    "\n",
    "Cool! Let's adjust the threshold to select 200 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_token = set()\n",
    "for star, values in feat_weights.items():\n",
    "    for token, weight in values:\n",
    "        # TODO: adjust the threshold to select 200 features\n",
    "        if abs(weight) > ???:\n",
    "            selected_token.add(token)\n",
    "        else:\n",
    "            break\n",
    "print(f'Number of tokens you selected: {len(selected_token)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(selected_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_SIZE = len(selected_token)\n",
    "print(f'Now feature size is {FEAT_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rebuild out feature map\n",
    "new_feat_map = {}\n",
    "for i, token in enumerate(selected_token):\n",
    "    new_feat_map[token] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-extract the features for training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = extract_feature(new_feat_map, review_list), star_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = extract_feature(new_feat_map, review_list_val), star_list_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you can re-run the Machine Learning toolkit section to rebuild the model using Scikit-Learn library, please mark down your previous results and compare the results with hashed features. The model evaluation functions is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Now we put all the evaluation process in a single function \n",
    "# Feel free to update and add more\n",
    "def validate_all(y_train, y_hat_train, y_val, y_hat_val):\n",
    "    print(f'Trainig accuracy is {accuracy_score(y_train, y_hat_train)}')\n",
    "    print(f'Validation accuracy is {accuracy_score(y_val, y_hat_val)}')\n",
    "    print()\n",
    "    print(f'Trainig RMSE is {np.sqrt(mean_squared_error(y_train, y_hat_train))}')\n",
    "    print(f'Validation RMSE is {np.sqrt(mean_squared_error(y_val, y_hat_val))}')\n",
    "    print()\n",
    "    print('Training set')\n",
    "    print(classification_report(y_train, y_hat_train))\n",
    "    print()\n",
    "    print('Validation set')\n",
    "    print(classification_report(y_val, y_hat_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerun the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l2', tol=0.0001, C=1.0, fit_intercept=True, solver='lbfgs', max_iter=30, \n",
    "                           multi_class='auto', verbose=1, n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model.predict(X_train)\n",
    "y_hat_val = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_all(y_train, y_hat_train, y_val, y_hat_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now using only 40% of the features, but you may find the model performance does not drop a lot. This is the power of feature selection, we kept the most important part for the model and significantly reduced the computation load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move on and try the new model we learned this week! The popular tree-based models! First, we need to regenerate the feature vectors withous normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = extract_feature(new_feat_map, review_list, normalize=True), star_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = extract_feature(new_feat_map, review_list_val, normalize=True), star_list_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through the scikit-learn API docs and think about what parameters you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add proper parameters for the random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=,\n",
    "                                  criterion=,\n",
    "                                  max_depth=, \n",
    "                                  min_samples_split=,\n",
    "                                  min_samples_leaf=, \n",
    "                                  max_features=, \n",
    "                                  n_jobs=, \n",
    "                                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = rf_model.predict(X_train)\n",
    "y_hat_val = rf_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_all(y_train, y_hat_train, y_val, y_hat_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the result match your expectation?\n",
    "\n",
    "Generaly speacking, Random Forest (RF) is non-linear model, which should have higher complexity than Logistic Regression (LR) - linear model. In other words, the RF should usually peforms better than LR. However, in our rating prediction problem using pure words, linear model is more approperate than tree model, so you please don't struggle if you cannot beat LR model. Instead, you should focus on how can you improve the model by tuning parameters.\n",
    "\n",
    "If the model performance is not as good as you may expect, think about what you can do to improve the model performance (based on the theory we learned during the video)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to do some experiments with:\n",
    "- num. of trees (n_estimators)\n",
    "- max_depth\n",
    "- max_features\n",
    "\n",
    "You should also track the training time for each choice of your parameter (see sample code below)\n",
    "\n",
    "Usually the min_samples_split and min_samples_leaf are also important, but given the volume of our training data, it's hard to pick the correct values, so we will just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "# ... your code here\n",
    "\n",
    "print(f'Time elapsed: {time() - start_time}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Decision Tree (GBDT)\n",
    "\n",
    "Now it's your time to play with GBDT, select the package you like and start to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Feature Engineering: Feature Hashing  (Optional for exploration)\n",
    "\n",
    "The performance of the baseline model is highly dependent on the **size of dictionary** and the **quality** of tokens. If we simply increase the size of dictionary (e.g., extend the size of dictionary from 500 to 5000), then the computational cost will increase and the model will also suffer from severe data sparsity, as the average length of reviews is much shorter than the full size of the dictionary. In order to improve the computational efficiency and utilize the sparsity of the data, we can use feature hashing (a.k.a hashing trick).\n",
    "\n",
    "Let the size of dictionary be 1000 in this model. Using a hashing function, we can map the 1000 tokens into a vector with a much smaller dimension (i.e., 300 dimensions in your implementation). That is, using a standard hash function we can compute the hash code for each token, and then take its value modulo 300 (e.g., hash(token) % 300). Finally, we take this value as the index for this token in the 300-dimension vector.\n",
    "\n",
    "It is obvious that there will be collisions in the 300-dimension vector, e.g., some different tokens will have the same index in the 300-dimension vector, meaning some information may be lost in the mapping. However, since our training instances are quite sparse, you will see this may not be a big problem in practice.\n",
    "\n",
    "Use this part of the code to replace original basic feature engineering code and rerun the model and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of hashing\n",
    "feat_hash = hash('like')\n",
    "feat_idx = feat_hash % FEAT_SIZE\n",
    "print(f'feature hash: {feat_hash}, feature index: {feat_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use feature hashing trick to re-do the feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the new feature exatraction function using feature hashing\n",
    "#       to construct feature vector for the given review list\n",
    "# Use this function to replace:\n",
    "# def extract_feature(feat_map: Dict[str, int], review_list: List[List[str]]) -> np.ndarray: ...\n",
    "def extract_feature_feat_hashing(review_list: List[List[str]]) -> np.ndarray:\n",
    "    n_sample, n_feat = len(review_list), FEAT_SIZE\n",
    "    X = np.zeros((n_sample, n_feat+1))\n",
    "    for i, tokens in enumerate(review_list):\n",
    "        feat = X[i]\n",
    "        for token in tokens:\n",
    "            if token in selected_token:\n",
    "                feat[hash(token) % FEAT_SIZE] += 1\n",
    "            else:\n",
    "                feat[n_feat] += 1\n",
    "        if np.sum(X[i]) > 0:\n",
    "            X[i] /= np.sum(X[i])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = extract_feature_feat_hashing(review_list), star_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = extract_feature_feat_hashing(review_list_val), star_list_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
